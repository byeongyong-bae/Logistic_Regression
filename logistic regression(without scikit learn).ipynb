{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "from functools import partial\n",
    "\n",
    "def mean(x):\n",
    "    return sum(x) / len(x)\n",
    "\n",
    "# degree of (obs - mean)\n",
    "def de_mean(x):\n",
    "    x_bar = mean(x)\n",
    "    return [x_i - x_bar for x_i in x]\n",
    "\n",
    "def sum_of_squares(x):\n",
    "    return sum(x_i ** 2 for x_i in x)\n",
    "\n",
    "def variance(x):\n",
    "    n = len(x)\n",
    "    deviations = de_mean(x)\n",
    "    return sum_of_squares(deviations) / (n - 1)\n",
    "\n",
    "def standard_deviation(x):\n",
    "    return math.sqrt(variance(x))\n",
    "\n",
    "def shape(M):\n",
    "    num_rows = len(M)\n",
    "    num_cols = len(M[0]) if M else 0\n",
    "    return num_rows, num_cols\n",
    "\n",
    "# jth value of each ith in i x j matrix\n",
    "def get_column(M, j):\n",
    "    return [M_i[j] for M_i in M]\n",
    "\n",
    "# x_1 * b_1 + ... + x_n * b_n\n",
    "def dot(x, b):\n",
    "    return sum(x_i * b_i\n",
    "               for x_i, b_i in zip(x, b))\n",
    "\n",
    "# get means and standard deviations of each column\n",
    "def scale(matrix):\n",
    "    num_rows, num_cols = shape(matrix)\n",
    "    means = [mean(get_column(matrix, j))\n",
    "             for j in range(num_cols)]\n",
    "    stdevs = [standard_deviation(get_column(matrix, j))\n",
    "              for j in range(num_cols)]\n",
    "    return means, stdevs\n",
    "\n",
    "# get matrix\n",
    "def make_matrix(num_rows, num_cols, entry):\n",
    "    return [[entry(i, j)\n",
    "             for j in range(num_cols)]\n",
    "            for i in range(num_rows)]\n",
    "\n",
    "# each column has mean 0 and standard deviation 1\n",
    "def rescale(matrix):\n",
    "    means, stdevs = scale(matrix)\n",
    "    def rescaled(i, j):\n",
    "        if stdevs[j] > 0:\n",
    "            return (matrix[i][j] - means[j]) / stdevs[j]\n",
    "        else:\n",
    "            return matrix[i][j]\n",
    "    num_rows, num_cols = shape(matrix)\n",
    "    return make_matrix(num_rows, num_cols, rescaled)\n",
    "\n",
    "def split_data(data, prob):\n",
    "    results = [], []\n",
    "    for row in data:\n",
    "        results[0 if random.random() < prob else 1].append(row)\n",
    "    return results\n",
    " \n",
    "def train_test_split(x, y, test_pct):\n",
    "    data = zip(x, y)\n",
    "    train, test = split_data(data, 1 - test_pct)\n",
    "    x_train, y_train = zip(*train)\n",
    "    x_test, y_test = zip(*test)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "# subtracts corresponding values\n",
    "def vector_subtract(x, b):\n",
    "    return [x_i - b_i for x_i, b_i in zip(x, b)]\n",
    "\n",
    "# scalar * vector\n",
    "def scalar_multiply(s, v):\n",
    "    return [s * v_i for v_i in v]\n",
    "\n",
    "# minimize stochastic\n",
    "def minimize_estimate(target, gradient, x, y, theta_0, alpha_0=0.01, safe=False):\n",
    "    data = zip(x, y)\n",
    "    \n",
    "    # initial value\n",
    "    theta = theta_0 \n",
    "    alpha = alpha_0\n",
    "    \n",
    "    # minimum so far\n",
    "    min_theta, min_value = None, float(\"inf\")\n",
    "    \n",
    "    iterations_with_no_improvement = 0\n",
    "    cnt_for_inf_loop = 0\n",
    "    \n",
    "    # limit 100\n",
    "    while ((iterations_with_no_improvement < 100) & (cnt_for_inf_loop < 1e10)):\n",
    "        cnt_for_inf_loop += 1\n",
    "        if safe:\n",
    "            if cnt_for_inf_loop > 1e5:\n",
    "                print('too much iteration')\n",
    "                break\n",
    "        value = sum(target(x_i, y_i, theta) for x_i, y_i in zip(x, y))\n",
    " \n",
    "        # if find new minimum, remeber it and go back to original step\n",
    "        if value < min_value:\n",
    "            min_theta, min_value = theta, value\n",
    "            if cnt_for_inf_loop % 10 == 1:\n",
    "                print('min_theta updates', min_theta)\n",
    "            iterations_with_no_improvement = 0\n",
    "            alpha = alpha_0\n",
    "        else:\n",
    "            # not improving, so reduce step size\n",
    "            iterations_with_no_improvement += 1\n",
    "            if (iterations_with_no_improvement % 10 == 5):\n",
    "                print(\"iterations_with_no_improvement is growing...\", iterations_with_no_improvement)\n",
    "            alpha *= 0.9\n",
    "\n",
    "        # take a gradient step for each of the data points\n",
    "        indexes = [i for i in range(len(x))];\n",
    "        random.shuffle(indexes)\n",
    "        for rand_i in indexes:\n",
    "            gradient_i = gradient(x[rand_i], y[rand_i], theta)\n",
    "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
    "    return min_theta\n",
    "\n",
    "# get function that for any input x returns -f(x)\n",
    "def negate(f):\n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
    " \n",
    "# same when returns list of numbers\n",
    "def negate_all(f):\n",
    "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]\n",
    "\n",
    "def maximize_estimate(target, gradient, x, y, theta_0, alpha_0=0.01):\n",
    "    return minimize_estimate(negate(target), negate_all(gradient), x, y, theta_0, alpha_0)\n",
    "\n",
    "\n",
    "\n",
    "# logistic\n",
    "# if x is big value, error occurs. create except syntax\n",
    "def logistic(x):\n",
    "    try:\n",
    "        return 1.0 / (1 + math.exp(-x))\n",
    "    except OverflowError:\n",
    "        return 1e-7\n",
    "\n",
    "\n",
    "# likehood\n",
    "# get each log likelihood\n",
    "def logistic_log_likelihood_i(x_i, y_i, beta):\n",
    "    if y_i == 1:\n",
    "        return math.log(logistic(dot(x_i, beta)))\n",
    "    else:\n",
    "        return math.log(1 - logistic(dot(x_i, beta)) + 1e-7)\n",
    "\n",
    "\n",
    "# cumsum each log likelihood\n",
    "def logistic_log_likelihood(x, y, beta):\n",
    "    return sum(logistic_log_likelihood_i(x_i, y_i, beta)\n",
    "               for x_i, y_i in zip(x, y))\n",
    "\n",
    "\n",
    "# get coefficient of 1~j\n",
    "def logistic_log_partial_ij(x_i, y_i, beta, j):\n",
    "    return (y_i - logistic(dot(x_i, beta))) * x_i[j]\n",
    "\n",
    "\n",
    "# vector of 1~j partial derivative\n",
    "def logistic_log_gradient_i(x_i, y_i, beta):\n",
    "    return [logistic_log_partial_ij(x_i, y_i, beta, j) for j, _ in enumerate(beta)]\n",
    "\n",
    "\n",
    "# vector sum about all data i\n",
    "def logistic_log_gradient(x, y, beta):\n",
    "    return np.sum(np.array([logistic_log_gradient_i(x_i, y_i, beta) for x_i, y_i in zip(x, y)]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_theta updates [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "min_theta updates [-0.828193491484706, 3.2679001064206012, -1.332241852694735, -0.3734153963116532, -0.4057459461229207, -0.17768430163388543, -0.08481046593726838, -0.3233633738798117, 0.06874152062906994]\n",
      "min_theta updates [-0.8082883615943327, 3.621972037122397, -1.61433299371704, -0.36354422465242925, -0.3565893950873303, -0.1946341159539385, -0.1434996366692612, -0.3386720186671833, 0.08973414764290577]\n",
      "iterations_with_no_improvement is growing... 5\n",
      "min_theta updates [-0.795612005172305, 3.7821770978819087, -1.7155587882173893, -0.3678116155562796, -0.3219024439780169, -0.21030133726665157, -0.13550710447109815, -0.3501709404667783, 0.08696788709497275]\n",
      "iterations_with_no_improvement is growing... 5\n",
      "iterations_with_no_improvement is growing... 5\n",
      "iterations_with_no_improvement is growing... 5\n",
      "iterations_with_no_improvement is growing... 5\n",
      "iterations_with_no_improvement is growing... 5\n",
      "iterations_with_no_improvement is growing... 5\n",
      "iterations_with_no_improvement is growing... 15\n",
      "iterations_with_no_improvement is growing... 5\n",
      "iterations_with_no_improvement is growing... 15\n",
      "iterations_with_no_improvement is growing... 5\n",
      "iterations_with_no_improvement is growing... 15\n",
      "iterations_with_no_improvement is growing... 5\n",
      "iterations_with_no_improvement is growing... 15\n",
      "iterations_with_no_improvement is growing... 5\n",
      "iterations_with_no_improvement is growing... 15\n",
      "iterations_with_no_improvement is growing... 25\n",
      "iterations_with_no_improvement is growing... 5\n",
      "iterations_with_no_improvement is growing... 15\n",
      "iterations_with_no_improvement is growing... 25\n",
      "iterations_with_no_improvement is growing... 35\n",
      "iterations_with_no_improvement is growing... 45\n",
      "iterations_with_no_improvement is growing... 5\n",
      "iterations_with_no_improvement is growing... 15\n",
      "iterations_with_no_improvement is growing... 25\n",
      "iterations_with_no_improvement is growing... 35\n",
      "iterations_with_no_improvement is growing... 5\n",
      "iterations_with_no_improvement is growing... 15\n",
      "iterations_with_no_improvement is growing... 25\n",
      "iterations_with_no_improvement is growing... 5\n",
      "iterations_with_no_improvement is growing... 15\n",
      "iterations_with_no_improvement is growing... 25\n",
      "iterations_with_no_improvement is growing... 5\n",
      "iterations_with_no_improvement is growing... 15\n",
      "iterations_with_no_improvement is growing... 25\n",
      "iterations_with_no_improvement is growing... 5\n",
      "iterations_with_no_improvement is growing... 15\n",
      "iterations_with_no_improvement is growing... 25\n",
      "iterations_with_no_improvement is growing... 35\n",
      "iterations_with_no_improvement is growing... 45\n",
      "iterations_with_no_improvement is growing... 55\n",
      "iterations_with_no_improvement is growing... 5\n",
      "iterations_with_no_improvement is growing... 15\n",
      "iterations_with_no_improvement is growing... 25\n",
      "iterations_with_no_improvement is growing... 35\n",
      "iterations_with_no_improvement is growing... 5\n",
      "iterations_with_no_improvement is growing... 15\n",
      "iterations_with_no_improvement is growing... 25\n",
      "iterations_with_no_improvement is growing... 5\n",
      "iterations_with_no_improvement is growing... 15\n",
      "iterations_with_no_improvement is growing... 25\n",
      "iterations_with_no_improvement is growing... 35\n",
      "iterations_with_no_improvement is growing... 45\n",
      "iterations_with_no_improvement is growing... 55\n",
      "iterations_with_no_improvement is growing... 65\n",
      "iterations_with_no_improvement is growing... 75\n",
      "iterations_with_no_improvement is growing... 85\n",
      "iterations_with_no_improvement is growing... 95\n",
      "beta [-0.7936168528739982, 3.791622781994152, -1.7212514974548747, -0.36863861464839, -0.32559477704051454, -0.21703522809307235, -0.14105608989215426, -0.36829542164243306, 0.0821232307776665]\n"
     ]
    }
   ],
   "source": [
    "random.seed(1030)\n",
    "\n",
    "data_frame = pd.read_csv('data.csv', engine='python')\n",
    "\n",
    "# split x,y in dataframe\n",
    "x_frame = data_frame.loc[:, 'hour_max':'sewer_Near']\n",
    "y_frame = data_frame['flooding']\n",
    "\n",
    "# x, y to list\n",
    "# 1 value is for intercept\n",
    "x = [[1] + list(row[:8]) for row in x_frame.values.tolist()]\n",
    "y = y_frame.values.tolist()\n",
    "\n",
    "# rescale\n",
    "x_rescale = rescale(x)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_rescale, y, 0.3)\n",
    "\n",
    "# max log likelihood of train data\n",
    "pt = partial(logistic_log_likelihood, x_train, y_train)\n",
    "gradient_pt = partial(logistic_log_gradient, x_train, y_train)\n",
    "\n",
    "# set starting point\n",
    "beta_0 = [1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# gradient descent\n",
    "beta_hat = maximize_estimate(logistic_log_likelihood_i,\n",
    "                           logistic_log_gradient_i,\n",
    "                           x_train, y_train, beta_0)\n",
    "\n",
    "print('beta', beta_hat)\n",
    "# flooding = -0.793 + 3.790hour_max - 1.719day_rain - 0.368slope - 0.325elevation - 0.216River_Near + 0.141rainT_Near - 0.366pump_Near + 0.081sewer_Near"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8668171557562077\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix and accuracy\n",
    "true_positives = false_positives = true_negatives = false_negatives = 0\n",
    "\n",
    "for x_i, y_i in zip(x_test, y_test):\n",
    "    predict = logistic(dot(beta_hat, x_i))\n",
    "\n",
    "    if y_i == 1 and predict >= 0.5:  # true positives\n",
    "        true_positives += 1\n",
    "    elif y_i == 1:                   # false negatives\n",
    "        false_negatives += 1\n",
    "    elif predict >= 0.5:             # false positives\n",
    "        false_positives += 1\n",
    "    else:                            # true negatives\n",
    "        true_negatives += 1\n",
    "\n",
    "accuracy = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
    "\n",
    "print(\"accuracy\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
